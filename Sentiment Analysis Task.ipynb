{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : A simple Sentiment Analysis task for classifying Yelp reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python package\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary are Unique words\n",
    "#Tokens are every single word that appears in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the english model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'I always uh do the main um processing, I mean, the uh um data-processing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "always\n",
      "uh\n",
      "do\n",
      "the\n",
      "main\n",
      "um\n",
      "processing\n",
      ",\n",
      "I\n",
      "mean\n",
      ",\n",
      "the\n",
      "uh\n",
      "um\n",
      "data\n",
      "-\n",
      "processing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in stats:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K.\n",
      "has\n",
      "a\n",
      "reasonable\n",
      "population\n"
     ]
    }
   ],
   "source": [
    "doc2 = 'U.K. has a reasonable population'\n",
    "stats = nlp(doc2)\n",
    "for token in stats:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "always\n",
      "uh\n",
      "do\n",
      "the\n",
      "main\n",
      "um\n",
      "processing\n",
      "I\n",
      "mean\n",
      "the\n",
      "uh\n",
      "um\n",
      "data\n",
      "processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in re.split('\\W+',doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "K\n",
      "has\n",
      "a\n",
      "reasonable\n",
      "population\n"
     ]
    }
   ],
   "source": [
    "for token in re.split('\\W+',doc2):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83814"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glands'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mfy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '  ',\n",
       " '!',\n",
       " '!!',\n",
       " '!!!',\n",
       " '!!!!',\n",
       " '!!!!!!!!!!!!!!!!',\n",
       " '!!!!.',\n",
       " '!!.',\n",
       " '!!?',\n",
       " '!!??',\n",
       " '!*',\n",
       " '!.',\n",
       " '!?',\n",
       " '!??',\n",
       " '\"',\n",
       " '\"\"',\n",
       " '#',\n",
       " \"##'s\",\n",
       " \"##'x\",\n",
       " \"#'s\",\n",
       " '#15',\n",
       " '#^%',\n",
       " '#dd',\n",
       " '$',\n",
       " '$19',\n",
       " '$Whose',\n",
       " '$Xxxxx',\n",
       " '$whose',\n",
       " '$xxxx',\n",
       " '%',\n",
       " '%-3',\n",
       " '%ach',\n",
       " '%ah',\n",
       " '%eh',\n",
       " '%er',\n",
       " '%ha',\n",
       " '%hm',\n",
       " '%huh',\n",
       " '%mm',\n",
       " '%oof',\n",
       " '%pw',\n",
       " '%uh',\n",
       " '%um',\n",
       " '%xx',\n",
       " '%xxx',\n",
       " '&',\n",
       " '&#',\n",
       " '&G.',\n",
       " '&L.',\n",
       " '&Ls',\n",
       " '&M.',\n",
       " '&P.',\n",
       " '&SA',\n",
       " '&T.',\n",
       " '&ex',\n",
       " '&in',\n",
       " '&ls',\n",
       " '&of',\n",
       " '&on',\n",
       " '&sa',\n",
       " '&the',\n",
       " '&to',\n",
       " '&uh',\n",
       " '&von',\n",
       " '&xx',\n",
       " '&xxx',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"''It\",\n",
       " \"''Xx\",\n",
       " \"''it\",\n",
       " \"''xx\",\n",
       " \"'-(\",\n",
       " \"'-)\",\n",
       " \"'03\",\n",
       " \"'07\",\n",
       " \"'20s\",\n",
       " \"'30s\",\n",
       " \"'40s\",\n",
       " \"'45\",\n",
       " \"'46\",\n",
       " \"'50s\",\n",
       " \"'60s\",\n",
       " \"'67\",\n",
       " \"'68\",\n",
       " \"'69\",\n",
       " \"'70's\",\n",
       " \"'70s\",\n",
       " \"'71\",\n",
       " \"'73\",\n",
       " \"'74\",\n",
       " \"'76\",\n",
       " \"'78\",\n",
       " \"'80\",\n",
       " \"'80's\",\n",
       " \"'80s\",\n",
       " \"'82\",\n",
       " \"'86\",\n",
       " \"'89\",\n",
       " \"'90's\",\n",
       " \"'90s\",\n",
       " \"'91\",\n",
       " \"'94\",\n",
       " \"'96\",\n",
       " \"'97\",\n",
       " \"'98\",\n",
       " \"'99\",\n",
       " \"'Arabi\",\n",
       " \"'Cause\",\n",
       " \"'Connery\",\n",
       " \"'Cos\",\n",
       " \"'Coz\",\n",
       " \"'Cuz\",\n",
       " \"'Id\",\n",
       " \"'Il\",\n",
       " \"'It\",\n",
       " \"'N\",\n",
       " \"'Nita\",\n",
       " \"'S\",\n",
       " \"'T\",\n",
       " \"'X\",\n",
       " \"'Xx\",\n",
       " \"'Xxx\",\n",
       " \"'Xxxx\",\n",
       " \"'Xxxxx\",\n",
       " \"'ai\",\n",
       " \"'al\",\n",
       " \"'am\",\n",
       " \"'amour\",\n",
       " \"'an\",\n",
       " \"'ao\",\n",
       " \"'arabi\",\n",
       " \"'bout\",\n",
       " \"'cause\",\n",
       " \"'connery\",\n",
       " \"'cos\",\n",
       " \"'coz\",\n",
       " \"'cuz\",\n",
       " \"'d\",\n",
       " \"'d.\",\n",
       " \"'dd\",\n",
       " \"'dd'x\",\n",
       " \"'ddx\",\n",
       " \"'droid\",\n",
       " \"'em\",\n",
       " \"'en\",\n",
       " \"'er\",\n",
       " \"'id\",\n",
       " \"'il\",\n",
       " \"'in\",\n",
       " \"'it\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'ma\",\n",
       " \"'n\",\n",
       " \"'n'\",\n",
       " \"'nita\",\n",
       " \"'ns\",\n",
       " \"'nt\",\n",
       " \"'nuff\",\n",
       " \"'re\",\n",
       " \"'recg\",\n",
       " \"'s\",\n",
       " \"'s**\",\n",
       " \"'s_\",\n",
       " \"'t\",\n",
       " \"'til\",\n",
       " \"'ts\",\n",
       " \"'uh\",\n",
       " \"'ve\",\n",
       " \"'x\",\n",
       " \"'x'\",\n",
       " \"'x**\",\n",
       " \"'xx\",\n",
       " \"'xxx\",\n",
       " \"'xxxx\",\n",
       " \"'y\",\n",
       " '(',\n",
       " '(((',\n",
       " '(*>',\n",
       " '(*_*)',\n",
       " '(-8',\n",
       " '(-:',\n",
       " '(-;',\n",
       " '(-_-)',\n",
       " '(-d',\n",
       " '(._.)',\n",
       " '(02)',\n",
       " '(:',\n",
       " '(;',\n",
       " '(=',\n",
       " '(>_<)',\n",
       " '(AM',\n",
       " '(^_^)',\n",
       " '(c)?Ìö]o?',\n",
       " '(c)?ìö]o?',\n",
       " '(c)x',\n",
       " '(dd)',\n",
       " '(o:',\n",
       " '(x)?Xx]x?',\n",
       " '(x)?xx]x?',\n",
       " '(x)x',\n",
       " '(x:',\n",
       " '(x_x)',\n",
       " '(¬_¬)',\n",
       " '(ಠ_ಠ)',\n",
       " '(╯°□°）╯︵┻━┻',\n",
       " ')',\n",
       " ')))',\n",
       " ')-:',\n",
       " ')/¯',\n",
       " '):',\n",
       " '*',\n",
       " '*$',\n",
       " '**',\n",
       " '***',\n",
       " '****',\n",
       " '*****',\n",
       " '*********',\n",
       " '**,',\n",
       " '**.',\n",
       " '**Anday**',\n",
       " '**Dougo',\n",
       " '**Dougo**',\n",
       " '**Eee**.',\n",
       " '**Mal**',\n",
       " '**Marchish**',\n",
       " '**X',\n",
       " '**Xxx**',\n",
       " '**Xxx**.',\n",
       " '**Xxxxx',\n",
       " '**Xxxxx**',\n",
       " '**Y',\n",
       " '**Yuh**',\n",
       " '**aimnay**',\n",
       " '**ainday**',\n",
       " '**anday**',\n",
       " '**arimay**',\n",
       " '**arimay**,',\n",
       " '**bisay**',\n",
       " '**bladyblah**',\n",
       " '**cooky**',\n",
       " '**doroter**.',\n",
       " '**dougo',\n",
       " '**dougo**',\n",
       " '**eee**.',\n",
       " '**ick**',\n",
       " '**icky**',\n",
       " '**junkmobile**',\n",
       " '**luch**',\n",
       " '**mal**',\n",
       " '**marchish**',\n",
       " '**maturer**',\n",
       " '**oray**,',\n",
       " '**ouchies**',\n",
       " '**ouchies**,',\n",
       " '**pammy**',\n",
       " '**pep**',\n",
       " '**piccy**',\n",
       " '**putty',\n",
       " '**ruthie**',\n",
       " '**shpritz**',\n",
       " '**staticky**',\n",
       " '**switcheroony**',\n",
       " '**thingy**',\n",
       " '**touristy**',\n",
       " '**uptay**',\n",
       " '**uyay**,',\n",
       " '**x',\n",
       " '**xxx**',\n",
       " '**xxx**.',\n",
       " '**xxxx',\n",
       " '**xxxx**',\n",
       " '**xxxx**,',\n",
       " '**xxxx**.',\n",
       " '**y',\n",
       " '**yuh**',\n",
       " '*1',\n",
       " '*2',\n",
       " '*3',\n",
       " '*4',\n",
       " '*ck',\n",
       " '*d',\n",
       " '*integrated',\n",
       " '*require*',\n",
       " '*when*',\n",
       " '*xxxx',\n",
       " '*xxxx*',\n",
       " '+',\n",
       " '+++',\n",
       " '+Appreciate',\n",
       " '+Broder',\n",
       " '+French',\n",
       " '+Killing',\n",
       " '+Saturday+',\n",
       " '+Schoema+',\n",
       " '+Xxxxx',\n",
       " '+Xxxxx+',\n",
       " '+an',\n",
       " '+appreciate',\n",
       " '+approaches',\n",
       " '+between',\n",
       " '+broder',\n",
       " '+conscience',\n",
       " '+constitution',\n",
       " '+french',\n",
       " '+grabbing',\n",
       " '+her',\n",
       " '+hierarchial',\n",
       " '+invariably',\n",
       " '+killing',\n",
       " '+offender',\n",
       " '+really',\n",
       " '+saturday+',\n",
       " '+schoema+',\n",
       " '+something',\n",
       " '+under',\n",
       " '+unknown+',\n",
       " '+wanted',\n",
       " '+xx',\n",
       " '+xxx',\n",
       " '+xxxx',\n",
       " '+xxxx+',\n",
       " ',',\n",
       " ',,',\n",
       " ',,,',\n",
       " ',,,,',\n",
       " ',,^',\n",
       " ',calif',\n",
       " ',xxxx',\n",
       " '-',\n",
       " \"-'m\",\n",
       " '-((',\n",
       " '-))',\n",
       " '--',\n",
       " '---',\n",
       " '----',\n",
       " '------',\n",
       " '-----------',\n",
       " '--------------',\n",
       " '------------------',\n",
       " '-------------------',\n",
       " '-----------------------',\n",
       " '------------------------',\n",
       " '-------------------------',\n",
       " '-----------------------------',\n",
       " '------------------------------',\n",
       " '-------------------------------',\n",
       " '--George',\n",
       " '--Xxxxx',\n",
       " '--george',\n",
       " '-/',\n",
       " '-0',\n",
       " '-0.06',\n",
       " '-10',\n",
       " '-11',\n",
       " '-12',\n",
       " '-13',\n",
       " '-14',\n",
       " '-15',\n",
       " '-16',\n",
       " '-17',\n",
       " '-18',\n",
       " '-20',\n",
       " '-21',\n",
       " '-22',\n",
       " '-24',\n",
       " '-27',\n",
       " '-2C',\n",
       " '-2s',\n",
       " '-3',\n",
       " '-30',\n",
       " '-36',\n",
       " '-37',\n",
       " '-40',\n",
       " '-47',\n",
       " '-50',\n",
       " '-52',\n",
       " '-5B',\n",
       " '-60',\n",
       " '-64',\n",
       " '-71',\n",
       " '-72',\n",
       " '-7B',\n",
       " '-8',\n",
       " '-80',\n",
       " '-87',\n",
       " '-D',\n",
       " '-LRB',\n",
       " '-LRB-',\n",
       " '-O',\n",
       " '-P',\n",
       " '-P1',\n",
       " '-RRB',\n",
       " '-RRB-',\n",
       " '-X',\n",
       " '-XXX',\n",
       " '-XXX-',\n",
       " '-_-',\n",
       " '-__-',\n",
       " '-considering',\n",
       " '-d',\n",
       " '-d.dd',\n",
       " '-ed',\n",
       " '-en',\n",
       " '-fi',\n",
       " '-lrb',\n",
       " '-lrb-',\n",
       " '-no',\n",
       " '-o',\n",
       " '-oh',\n",
       " '-op',\n",
       " '-p',\n",
       " '-p1',\n",
       " '-rrb',\n",
       " '-rrb-',\n",
       " '-uh',\n",
       " '-up',\n",
       " '-x',\n",
       " '-xxx',\n",
       " '-xxxx',\n",
       " '-|',\n",
       " '.',\n",
       " '.!!',\n",
       " '.!*',\n",
       " '.-',\n",
       " '..',\n",
       " '..!',\n",
       " '..!!',\n",
       " '..!!.',\n",
       " '..!*',\n",
       " '...',\n",
       " '...!',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '.......',\n",
       " '........',\n",
       " '..........',\n",
       " '...........',\n",
       " '...................',\n",
       " '......................',\n",
       " '....?',\n",
       " '..>',\n",
       " '..?',\n",
       " '.00',\n",
       " '.01',\n",
       " '.02',\n",
       " '.03',\n",
       " '.04',\n",
       " '.05',\n",
       " '.06',\n",
       " '.07',\n",
       " '.08',\n",
       " '.09',\n",
       " '.10',\n",
       " '.11',\n",
       " '.12',\n",
       " '.13',\n",
       " '.14',\n",
       " '.15',\n",
       " '.16',\n",
       " '.17',\n",
       " '.18',\n",
       " '.19',\n",
       " '.20',\n",
       " '.21',\n",
       " '.22',\n",
       " '.23',\n",
       " '.24',\n",
       " '.25',\n",
       " '.26',\n",
       " '.27',\n",
       " '.270',\n",
       " '.28',\n",
       " '.29',\n",
       " '.30',\n",
       " '.31',\n",
       " '.32',\n",
       " '.33',\n",
       " '.34',\n",
       " '.342',\n",
       " '.35',\n",
       " '.36',\n",
       " '.37',\n",
       " '.38',\n",
       " '.39',\n",
       " '.3V',\n",
       " '.3v',\n",
       " '.4',\n",
       " '.40',\n",
       " '.41',\n",
       " '.419',\n",
       " '.42',\n",
       " '.43',\n",
       " '.44',\n",
       " '.45',\n",
       " '.46',\n",
       " '.47',\n",
       " '.48',\n",
       " '.49',\n",
       " '.50',\n",
       " '.51',\n",
       " '.52',\n",
       " '.53',\n",
       " '.54',\n",
       " '.55',\n",
       " '.56',\n",
       " '.57',\n",
       " '.58',\n",
       " '.59',\n",
       " '.60',\n",
       " '.61',\n",
       " '.62',\n",
       " '.63',\n",
       " '.64',\n",
       " '.65',\n",
       " '.66',\n",
       " '.67',\n",
       " '.68',\n",
       " '.69',\n",
       " '.70',\n",
       " '.71',\n",
       " '.72',\n",
       " '.73',\n",
       " '.74',\n",
       " '.75',\n",
       " '.76',\n",
       " '.77',\n",
       " '.78',\n",
       " '.79',\n",
       " '.80',\n",
       " '.81',\n",
       " '.82',\n",
       " '.83',\n",
       " '.84',\n",
       " '.85',\n",
       " '.86',\n",
       " '.87',\n",
       " '.88',\n",
       " '.89',\n",
       " '.9.82',\n",
       " '.90',\n",
       " '.91',\n",
       " '.92',\n",
       " '.93',\n",
       " '.94',\n",
       " '.95',\n",
       " '.96',\n",
       " '.97',\n",
       " '.98',\n",
       " '.99',\n",
       " '.?',\n",
       " '.A.',\n",
       " '.B.',\n",
       " '.C.',\n",
       " '.D.',\n",
       " '.E.',\n",
       " '.F.',\n",
       " '.G.',\n",
       " '.H.',\n",
       " '.I.',\n",
       " '.J.',\n",
       " '.K.',\n",
       " '.L.',\n",
       " '.LBR',\n",
       " '.M.',\n",
       " '.N.',\n",
       " '.O.',\n",
       " '.P.',\n",
       " '.R.',\n",
       " '.S.',\n",
       " '.T.',\n",
       " '.V.',\n",
       " '.Va',\n",
       " '.W.',\n",
       " '.XXX',\n",
       " '.Y.',\n",
       " '._.',\n",
       " '.a.',\n",
       " '.b.',\n",
       " '.ba',\n",
       " '.c.',\n",
       " '.cn',\n",
       " '.d',\n",
       " '.d.',\n",
       " '.d.dd',\n",
       " '.dd',\n",
       " '.ddd',\n",
       " '.e.',\n",
       " '.f.',\n",
       " '.g.',\n",
       " '.h.',\n",
       " '.i.',\n",
       " '.j.',\n",
       " '.k.',\n",
       " '.l.',\n",
       " '.lbr',\n",
       " '.m.',\n",
       " '.n.',\n",
       " '.o.',\n",
       " '.p.',\n",
       " '.r.',\n",
       " '.s.',\n",
       " '.t.',\n",
       " '.tw',\n",
       " '.us',\n",
       " '.v.',\n",
       " '.va',\n",
       " '.w.',\n",
       " '.what',\n",
       " '.xxx',\n",
       " '.xxxx',\n",
       " '.y.',\n",
       " '/',\n",
       " '/-',\n",
       " '/.',\n",
       " '//',\n",
       " '/11',\n",
       " '/15',\n",
       " '/16',\n",
       " '/18',\n",
       " '/20',\n",
       " '/3',\n",
       " '/30',\n",
       " '/31',\n",
       " '/32',\n",
       " '/50',\n",
       " '/?',\n",
       " '/d',\n",
       " '/foreign',\n",
       " '/or',\n",
       " '/xxxx',\n",
       " '0',\n",
       " \"0's\",\n",
       " '0.',\n",
       " '0.0',\n",
       " '0.0002',\n",
       " '0.0015',\n",
       " '0.01',\n",
       " '0.02',\n",
       " '0.025',\n",
       " '0.03',\n",
       " '0.0343',\n",
       " '0.04',\n",
       " '0.05',\n",
       " '0.07',\n",
       " '0.1',\n",
       " '0.10',\n",
       " '0.12',\n",
       " '0.13',\n",
       " '0.14',\n",
       " '0.15',\n",
       " '0.16',\n",
       " '0.17',\n",
       " '0.19',\n",
       " '0.2',\n",
       " '0.20',\n",
       " '0.22',\n",
       " '0.23',\n",
       " '0.24',\n",
       " '0.25',\n",
       " '0.26',\n",
       " '0.272',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.32',\n",
       " '0.35',\n",
       " '0.37',\n",
       " '0.4',\n",
       " '0.43',\n",
       " '0.45',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.53',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.59',\n",
       " '0.6',\n",
       " '0.60',\n",
       " '0.63',\n",
       " '0.65',\n",
       " '0.7',\n",
       " '0.70',\n",
       " '0.75',\n",
       " '0.8',\n",
       " '0.9',\n",
       " '0.91',\n",
       " '0.94',\n",
       " '0.97',\n",
       " '0.99',\n",
       " '0.o',\n",
       " '00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000',\n",
       " '001',\n",
       " '002',\n",
       " '003',\n",
       " '004',\n",
       " '0044',\n",
       " '005',\n",
       " '006',\n",
       " '007',\n",
       " '008',\n",
       " '009',\n",
       " '00949',\n",
       " '00:39:42',\n",
       " '00:42:30',\n",
       " '00m',\n",
       " '00s',\n",
       " '00x',\n",
       " '01',\n",
       " '01/13/2007',\n",
       " '010',\n",
       " '011',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '015',\n",
       " '016',\n",
       " '018',\n",
       " '019',\n",
       " '01:45',\n",
       " '02',\n",
       " '02)',\n",
       " '020',\n",
       " '021',\n",
       " '023',\n",
       " '024',\n",
       " '025',\n",
       " '027',\n",
       " '028',\n",
       " '029',\n",
       " '02:25',\n",
       " '03',\n",
       " '030',\n",
       " '031',\n",
       " '032',\n",
       " '033',\n",
       " '035',\n",
       " '039',\n",
       " '0390',\n",
       " '03]',\n",
       " '04',\n",
       " '040',\n",
       " '043',\n",
       " '044',\n",
       " '046',\n",
       " '04s',\n",
       " '050',\n",
       " '0516',\n",
       " '053',\n",
       " '055',\n",
       " '056',\n",
       " '0566',\n",
       " '057',\n",
       " '059',\n",
       " '05:15:41',\n",
       " '060',\n",
       " '061',\n",
       " '062',\n",
       " '0622',\n",
       " '064',\n",
       " '065',\n",
       " '066',\n",
       " '067',\n",
       " '069',\n",
       " '06]',\n",
       " '07',\n",
       " '070',\n",
       " '072',\n",
       " '073',\n",
       " '074',\n",
       " '075',\n",
       " '076',\n",
       " '077',\n",
       " '078',\n",
       " '07:57:27',\n",
       " '08',\n",
       " '080',\n",
       " '0845',\n",
       " '08457',\n",
       " '085',\n",
       " '086',\n",
       " '0861',\n",
       " '087',\n",
       " '088',\n",
       " '089',\n",
       " '09',\n",
       " '09.',\n",
       " '090',\n",
       " '092',\n",
       " '094',\n",
       " '096',\n",
       " '098',\n",
       " '099',\n",
       " '09:00',\n",
       " '09:00:15',\n",
       " '09:30',\n",
       " '09:51:22',\n",
       " '09E',\n",
       " '09e',\n",
       " '0:00',\n",
       " '0:9',\n",
       " '0AM',\n",
       " '0ER',\n",
       " '0PM',\n",
       " '0SX',\n",
       " '0ZX',\n",
       " '0_0',\n",
       " '0_b',\n",
       " '0_o',\n",
       " '0am',\n",
       " '0er',\n",
       " '0mA',\n",
       " '0ma',\n",
       " '0pm',\n",
       " '0sx',\n",
       " '0th',\n",
       " '0zx',\n",
       " '1',\n",
       " '1(k',\n",
       " '1,000',\n",
       " '1,000,000',\n",
       " '1,000:8.40',\n",
       " '1,000:8.55',\n",
       " '1,001',\n",
       " '1,003,884',\n",
       " '1,012',\n",
       " '1,013',\n",
       " '1,014',\n",
       " '1,022,000',\n",
       " '1,026.46',\n",
       " '1,027',\n",
       " '1,030',\n",
       " '1,040',\n",
       " '1,048,500,000',\n",
       " '1,050',\n",
       " '1,050,000',\n",
       " '1,070,000',\n",
       " '1,074',\n",
       " '1,100',\n",
       " '1,103.11',\n",
       " '1,118',\n",
       " '1,120,317',\n",
       " '1,124',\n",
       " '1,150',\n",
       " '1,150,000',\n",
       " '1,155',\n",
       " '1,173.8',\n",
       " '1,178',\n",
       " '1,200',\n",
       " '1,200,000',\n",
       " '1,214',\n",
       " '1,224',\n",
       " '1,235',\n",
       " '1,240',\n",
       " '1,244',\n",
       " '1,250',\n",
       " '1,250,000',\n",
       " '1,263,000',\n",
       " '1,271',\n",
       " '1,275,000',\n",
       " '1,290',\n",
       " '1,296,000',\n",
       " '1,296,800',\n",
       " '1,298',\n",
       " '1,300',\n",
       " '1,300,000',\n",
       " '1,310',\n",
       " '1,320',\n",
       " '1,327',\n",
       " '1,350',\n",
       " '1,350,000',\n",
       " '1,365,226',\n",
       " '1,368',\n",
       " '1,376',\n",
       " '1,400',\n",
       " '1,400,000',\n",
       " '1,425,035',\n",
       " '1,430',\n",
       " '1,450',\n",
       " '1,450,635',\n",
       " '1,458,000',\n",
       " '1,474',\n",
       " '1,475,000',\n",
       " '1,480',\n",
       " '1,500',\n",
       " '1,500,000',\n",
       " '1,502',\n",
       " '1,531,000',\n",
       " '1,534',\n",
       " '1,555',\n",
       " '1,580',\n",
       " '1,600',\n",
       " '1,616,000',\n",
       " '1,620',\n",
       " '1,640',\n",
       " '1,642',\n",
       " '1,647',\n",
       " '1,685',\n",
       " '1,695,000',\n",
       " '1,700',\n",
       " '1,704',\n",
       " '1,716',\n",
       " '1,730',\n",
       " '1,735',\n",
       " '1,750',\n",
       " '1,770',\n",
       " '1,784',\n",
       " '1,784,400',\n",
       " '1,800',\n",
       " '1,802,000',\n",
       " '1,809,300',\n",
       " '1,810,700',\n",
       " '1,816,000',\n",
       " '1,824',\n",
       " '1,828,000',\n",
       " '1,838,200',\n",
       " '1,843,000',\n",
       " '1,848,000',\n",
       " '1,850',\n",
       " '1,859',\n",
       " '1,878',\n",
       " '1,892',\n",
       " '1,900',\n",
       " '1,908',\n",
       " '1,920',\n",
       " '1,973',\n",
       " '1,977',\n",
       " '1,979,000',\n",
       " '1-',\n",
       " '1-202-225-3121',\n",
       " '1-800-453-9000',\n",
       " '1-800-660-1350',\n",
       " '1-877-851-6437',\n",
       " '1-945-220-0044',\n",
       " '1.',\n",
       " '1.0',\n",
       " '1.001',\n",
       " '1.01',\n",
       " '1.011',\n",
       " '1.02',\n",
       " '1.03',\n",
       " '1.04',\n",
       " '1.05',\n",
       " '1.06',\n",
       " '1.07',\n",
       " '1.08',\n",
       " '1.09',\n",
       " '1.092',\n",
       " '1.1',\n",
       " '1.10',\n",
       " '1.11',\n",
       " '1.12',\n",
       " '1.125',\n",
       " '1.1270',\n",
       " '1.1280',\n",
       " '1.13',\n",
       " '1.130',\n",
       " '1.14',\n",
       " '1.143',\n",
       " '1.15',\n",
       " '1.1510',\n",
       " '1.1580',\n",
       " '1.16',\n",
       " '1.1650',\n",
       " '1.168',\n",
       " '1.17',\n",
       " '1.175',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.1960',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.22',\n",
       " '1.23',\n",
       " '1.234',\n",
       " '1.2345',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.255',\n",
       " '1.26',\n",
       " '1.2645',\n",
       " '1.27',\n",
       " '1.2745',\n",
       " '1.2795',\n",
       " '1.283',\n",
       " '1.29',\n",
       " '1.2965',\n",
       " '1.3',\n",
       " '1.30',\n",
       " '1.31',\n",
       " '1.32',\n",
       " '1.34',\n",
       " '1.35',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review\n",
       "0  negative  terrible place to work for i just heard a stor...\n",
       "1  negative   hours , minutes total time for an extremely s...\n",
       "2  negative  my less than stellar review is for service . w...\n",
       "3  negative  i m granting one star because there s no way t...\n",
       "4  negative  the food here is mediocre at best . i went aft..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55995</th>\n",
       "      <td>positive</td>\n",
       "      <td>great food . wonderful , friendly service . i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55996</th>\n",
       "      <td>positive</td>\n",
       "      <td>charlotte should be the new standard for moder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55997</th>\n",
       "      <td>positive</td>\n",
       "      <td>get the encore sandwich ! ! make sure to get i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55998</th>\n",
       "      <td>positive</td>\n",
       "      <td>i m a pretty big ice cream gelato fan . pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55999</th>\n",
       "      <td>positive</td>\n",
       "      <td>where else can you find all the parts and piec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                             review\n",
       "55995  positive  great food . wonderful , friendly service . i ...\n",
       "55996  positive  charlotte should be the new standard for moder...\n",
       "55997  positive  get the encore sandwich ! ! make sure to get i...\n",
       "55998  positive  i m a pretty big ice cream gelato fan . pretty...\n",
       "55999  positive  where else can you find all the parts and piec..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVocabulary():\n",
    "    unkToken = '<UNK>'\n",
    "    vocab['t_2_i'] = {}\n",
    "    vocab['i_2_t'] = {}\n",
    "    vocab['unkToken'] = unkToken\n",
    "    idx = addToken(unkToken)\n",
    "    vocab['unkTokenIdx'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToken(token):\n",
    "    if token in vocab['t_2_i']:\n",
    "        idx = vocab['t_2_i'][token]\n",
    "    else:\n",
    "        idx = len(vocab['t_2_i'])\n",
    "        vocab['t_2_i'][token] = idx\n",
    "        vocab['i_2_t'][idx] = token\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addManyTokens(tokens):\n",
    "    idxes = [addToken(token) for token in tokens]\n",
    "    return idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to check for the Token\n",
    "\n",
    "def lookUpToken(token):\n",
    "    if vocab['unkTokenIdx']>=0:\n",
    "        return vocab['t_2_i'].get(token,vocab['unkTokenIdx'])\n",
    "    else:\n",
    "        return vocab['t_2_i'][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for checking the index of a certain token\n",
    "def lookUpIndex(idx):\n",
    "    if idx not in vocab['i_2_t']:\n",
    "        raise KeyError(\"the index (%d) is not there\" % idx)\n",
    "    return vocab['i_2_t'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing on more frequent words.i.e Only words occuring more than 25 are the ones which will be added as tokens.\n",
    "def vocabularyFromDataFrame(df,cutoff=25):\n",
    "    initializeVocabulary()\n",
    "    wordCounts = Counter()\n",
    "    for r in df.review:\n",
    "        for word in re.split('\\W+',r):\n",
    "            wordCounts[word] += 1\n",
    "    for word,count in wordCounts.items():\n",
    "        if count > cutoff:\n",
    "            addToken(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabularyFromCorpus(Corpus,cutoff=25):\n",
    "    initializeVocabulary()\n",
    "    wordCounts = Counter()\n",
    "    for doc in Corpus:\n",
    "        for word in re.split('\\W+',doc):\n",
    "            wordCounts[word] += 1\n",
    "    for word,count in wordCounts.items():\n",
    "        if count > cutoff:\n",
    "            addToken(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabularyFromDataFrame(df)\n",
    "Corpus = np.asarray(df['review'])\n",
    "vocabularyFromCorpus(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['terrible place to work for i just heard a story of them find a girl over her biological father coming in there who she hadn t seen in years she said hi to him which upset his wife and they left she finished the rest of her day working fine the next day when she went into work they fired over that situation . i for one and boycotting texas roadhouse because any place that could be that cruel to their staff does not deserve my business . . . yelp wants me to give them a star but i don t believe they deserve it',\n",
       "       ' hours , minutes total time for an extremely simple physical . stay away unless you have hours to waste ! ! ! ',\n",
       "       'my less than stellar review is for service . we waited minutes for our meals to be delivered . when we questioned the waiter , he was not helpful , so we asked to speak to the manager . the manager did not even come to speak with us ! we were loyal neighborhood customers , even walking to the restaurant frequently ! my husband then wrote an email to the owner , and it was ignored by him as well . this is obviously a business that does not value customer service , we were very disappointed and eastwind has lost regular customers',\n",
       "       'i m granting one star because there s no way to give none . n nas i write this , the door to my room is open , and two burly gentlemen are drilling and hammering and drilling and hammering i just checked , and it s going to go on all day . evidently , the inn is installing a new lock system , and for some reason it was impossible to wait until i checked out later today . counting on solitude and quiet , i d brought work to do , but it s now utterly impossible to concentrate . i stayed here once before and loved it , but this lack of concern and respect sets a a new low . ',\n",
       "       'the food here is mediocre at best . i went after reading all the above positive reviews and was really disappointed . the service was great , hence the stars , but my relleno was soggy and my tamele looked tasted like it was made in a factory . the guacamole was decent and at i didn t care that the margarita tasted like a mix . '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookUpToken('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookUpIndex(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8946"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab['t_2_i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing tokens in a numerical format since ML models don't work with \n",
    "#Categorical values\n",
    "\n",
    "#The helper function is operating in a way that it assigns the token number index \n",
    "#in the generated matrix of zeros , in order to make sure that different tokens possess different \n",
    "#arrays (Main purpose for the oneHotVector encoding)\n",
    "\n",
    "def oneHotVector(token,N):\n",
    "    oneHot = np.zeros((N,1))\n",
    "    oneHot[lookUpToken(token)] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(vocab['t_2_i'])\n",
    "token = 'the'\n",
    "oneHot = oneHotVector(token,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "for token in vocab['t_2_i']:\n",
    "    print(token)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A helper function that computes the feature vector of the whole document for the \n",
    "# whole review\n",
    "def computeFeatures(doc,N):\n",
    "    isFirst = True\n",
    "    for token in doc:\n",
    "        oneHot = oneHotVector(token,N)\n",
    "        if isFirst:\n",
    "            xF = oneHot\n",
    "            isFirst = False\n",
    "        else:\n",
    "            xF = np.hstack((xF,oneHot))\n",
    "    return np.mean(xF,axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.66666667],\n",
       "       [0.33333333]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3,1))\n",
    "a[2]=1\n",
    "b = np.zeros((3,1))\n",
    "b[1]=1\n",
    "c = np.zeros((3,1))\n",
    "c[1]=1\n",
    "X = np.hstack((a,b,c))\n",
    "np.mean(X,axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b[:,0]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26027397],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       ...,\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # computeFeatures()\n",
    "# L = len(doc)\n",
    "computeFeatures(doc,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeatures_fast(doc,N):\n",
    "    fv = np.zeros(N)\n",
    "    numTokens = 0\n",
    "    for token in doc:\n",
    "        fv[lookUpToken(token)] += 1\n",
    "        numTokens += 1\n",
    "    return fv/numTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToFeatureMatrix(Corpus,N):\n",
    "    isFirst = True\n",
    "    for doc in Corpus:\n",
    "        fv = computeFeatures(doc,N)\n",
    "        if isFirst:\n",
    "            fM = fv\n",
    "            isFirst = False\n",
    "        else:\n",
    "            fM = np.hstack((fM,fv))\n",
    "    return fM.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToFeatureMatrix_fast(Corpus,N):\n",
    "    fM = np.zeros((N,len(Corpus)))\n",
    "    i = 0\n",
    "    for doc in Corpus:\n",
    "        fM[:,i] = computeFeatures_fast(doc,N)\n",
    "        i+=1\n",
    "    return fM.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 µs ± 17.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fv = computeFeatures_fast(Corpus[0],len(vocab['t_2_i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.37 s ± 289 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fv = computeFeatures(Corpus[0],len(vocab['t_2_i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "X = np.asarray(df['review'])\n",
    "y = np.asarray(df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyFromCorpus(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(vocab['t_2_i'])\n",
    "Xtrain_fM = corpusToFeatureMatrix_fast(Xtrain,N)\n",
    "Xtest_fM = corpusToFeatureMatrix_fast(Xtest,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39200, 7341)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_fM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16800, 7341)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_fM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression as clf\n",
    "#from sklearn.naive_bayes import GaussianNB as clf\n",
    "#from sklearn.ensemble import RandomForestClassifier as clf\n",
    "from sklearn.svm import SVC as clf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = clf().fit(Xtrain_fM,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = M.predict(Xtest_fM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(ytest,y_pred)\n",
    "sns.heatmap(mat.T,square=True,annot=True,fmt='d',cbar=False,\n",
    "           xticklabels=np.unique(y),yticklabels=np.unique(y))\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
